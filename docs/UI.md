# DocuChat Gradio Web UI Project Documentation

## Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture Overview](#architecture-overview)
3. [Implementation Phases](#implementation-phases)
4. [Technical Integration Points](#technical-integration-points)
5. [UI/UX Design Specifications](#uiux-design-specifications)
6. [Deployment and Maintenance](#deployment-and-maintenance)
7. [Risk Assessment and Mitigation](#risk-assessment-and-mitigation)
8. [Development Guidelines](#development-guidelines)
9. [Testing Strategy](#testing-strategy)
10. [Performance Considerations](#performance-considerations)

---

## 1. Project Overview

### 1.1 Project Description
The DocuChat Gradio Web UI project extends the existing CLI-based RAG system with a modern, user-friendly web interface built using Gradio. This transformation maintains all core functionality while providing an accessible web-based interface for document processing, management, and interactive Q&A sessions.

### 1.2 Project Goals
- **Primary Goal**: Transform the CLI-based DocuChat into a web-accessible application
- **User Experience**: Provide intuitive document upload, processing, and chat interfaces
- **Maintain Performance**: Preserve the existing RAG pipeline's efficiency and accuracy
- **Security**: Implement robust security measures for web deployment
- **Scalability**: Design for future enhancements and multi-user support

### 1.3 Current System Analysis
The existing DocuChat system comprises:
```
DocuChat/
â”œâ”€â”€ docuchat.py              # Main CLI entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py               # Command-line interface
â”‚   â”œâ”€â”€ document_loader.py   # PDF/DOCX/TXT processing
â”‚   â”œâ”€â”€ chunker.py          # Text chunking logic
â”‚   â”œâ”€â”€ embeddings.py       # Sentence-transformer embeddings
â”‚   â”œâ”€â”€ vector_db.py        # ChromaDB integration
â”‚   â”œâ”€â”€ llm_client.py       # Ollama HTTP API client
â”‚   â”œâ”€â”€ rag_pipeline.py     # Complete RAG orchestration
â”‚   â”œâ”€â”€ progress_manager.py # Progress tracking utilities
â”‚   â””â”€â”€ simple_timer.py     # Performance timing
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ chroma/                # Vector database storage
```

### 1.4 Technology Stack
**Existing Components:**
- Python 3.8+
- Sentence Transformers (all-MiniLM-L6-v2)
- ChromaDB for vector storage
- Ollama with gemma3:270m model
- PyPDF, python-docx for document processing

**New Web UI Components:**
- Gradio 4.x for web interface
- FastAPI for backend API (optional advanced features)
- WebSocket support for real-time updates
- Session management for multi-user support

---

## 2. Architecture Overview

### 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Web Browser Interface                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      Gradio Frontend                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Document Upload â”‚ â”‚ Processing View â”‚ â”‚   Chat Interfaceâ”‚â”‚
â”‚  â”‚    Interface    â”‚ â”‚   & Monitoring  â”‚ â”‚   & History     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Gradio Backend                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Session Management Layer                   â”‚â”‚
â”‚  â”‚  - User sessions  - Upload tracking  - Chat history    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Existing RAG Pipeline                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ DocumentLoaderâ”‚ â”‚  Chunker     â”‚ â”‚  EmbeddingGenerator  â”‚â”‚
â”‚  â”‚   (.py/.docx/â”‚ â”‚ (2000 chars  â”‚ â”‚   (all-MiniLM-L6-v2) â”‚â”‚
â”‚  â”‚    .txt)     â”‚ â”‚ w/ 200 overlap)â”‚ â”‚                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  VectorDB    â”‚ â”‚  LLMClient   â”‚ â”‚    RAGPipeline       â”‚â”‚
â”‚  â”‚ (ChromaDB)   â”‚ â”‚  (Ollama     â”‚ â”‚  (Complete Orchestr.)â”‚â”‚
â”‚  â”‚              â”‚ â”‚ gemma3:270m) â”‚ â”‚                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        Storage Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚    ChromaDB     â”‚ â”‚  File Storage   â”‚ â”‚  Session Data   â”‚â”‚
â”‚  â”‚ (Vector Store)  â”‚ â”‚ (Documents)     â”‚ â”‚   (Redis?)      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Component Interaction Flow

```
User Upload â†’ File Validation â†’ Document Processing Pipeline
     â”‚                               â”‚
     v                               v
Session Storage â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Vector Database
     â”‚                               â”‚
     v                               v
Web Interface â†â”€â”€â”€â”€ RAG Query â†â”€â”€â”€â”€ Embedding Search
     â”‚                               â”‚
     v                               v
Chat Display â†â”€â”€â”€â”€ LLM Response â†â”€â”€â”€ Context Assembly
```

### 2.3 Data Flow Patterns

**Document Processing Flow:**
1. User uploads documents via web interface
2. Files validated and temporarily stored
3. Background processing through existing pipeline
4. Real-time progress updates via WebSocket
5. Results stored and indexed in session

**Query Processing Flow:**
1. User submits question via chat interface
2. Question embedded using existing EmbeddingGenerator
3. Vector search performed on ChromaDB
4. Context assembled and sent to Ollama
5. Response streamed back to user interface

---

## 3. Implementation Phases

### Phase 1: Core Web Interface Foundation (Weeks 1-2)

#### 3.1.1 Objectives
- Create basic Gradio application structure
- Implement document upload functionality
- Integrate existing DocumentLoader component
- Establish session management foundation

#### 3.1.2 Deliverables

**File: `src/web_ui.py`**
```python
import gradio as gr
import tempfile
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import uuid
import json
from datetime import datetime

from document_loader import DocumentLoader
from progress_manager import create_progress_manager

class DocuChatWebUI:
    def __init__(self):
        self.document_loader = DocumentLoader(verbose=False)
        self.sessions = {}  # Session ID -> Session data
        self.upload_dir = Path("./uploads")
        self.upload_dir.mkdir(exist_ok=True)
    
    def create_session(self) -> str:
        """Create new user session"""
        session_id = str(uuid.uuid4())
        self.sessions[session_id] = {
            'created_at': datetime.now(),
            'documents': [],
            'processing_status': 'idle',
            'chat_history': []
        }
        return session_id
    
    def upload_documents(self, files: List[gr.File], session_id: str) -> Tuple[str, List[str]]:
        """Handle document upload"""
        if not files:
            return "No files uploaded", []
        
        uploaded_files = []
        session_dir = self.upload_dir / session_id
        session_dir.mkdir(exist_ok=True)
        
        for file in files:
            if file is not None:
                # Copy to session directory
                dest_path = session_dir / file.name
                with open(dest_path, 'wb') as f:
                    f.write(file.read())
                uploaded_files.append(str(dest_path))
        
        # Update session
        if session_id in self.sessions:
            self.sessions[session_id]['documents'].extend(uploaded_files)
        
        return f"Uploaded {len(uploaded_files)} files successfully", uploaded_files

# Gradio Interface Definition
def create_interface():
    ui = DocuChatWebUI()
    
    with gr.Blocks(title="DocuChat Web UI", theme=gr.themes.Soft()) as interface:
        # Session state
        session_id = gr.State(value=ui.create_session)
        
        gr.Markdown("# DocuChat - Document Q&A System")
        gr.Markdown("Upload your documents and ask questions about their content.")
        
        with gr.Tab("Document Upload"):
            file_upload = gr.Files(
                label="Upload Documents",
                file_types=[".pdf", ".docx", ".txt", ".md"],
                file_count="multiple"
            )
            
            upload_btn = gr.Button("Process Documents", variant="primary")
            upload_status = gr.Textbox(label="Status", interactive=False)
            document_list = gr.Dataframe(
                headers=["Filename", "Size", "Status"],
                label="Uploaded Documents"
            )
        
        with gr.Tab("Chat Interface"):
            gr.Markdown("*Upload and process documents first*")
            # Chat interface will be implemented in Phase 2
        
        # Event handlers
        upload_btn.click(
            fn=ui.upload_documents,
            inputs=[file_upload, session_id],
            outputs=[upload_status, document_list]
        )
    
    return interface

if __name__ == "__main__":
    interface = create_interface()
    interface.launch(server_name="0.0.0.0", server_port=7860, share=False)
```

**File: `src/web_session.py`**
```python
"""
Session management for DocuChat Web UI.
Handles user sessions, file uploads, and processing state.
"""

import json
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import threading
import time

class WebSession:
    """Individual user session management"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.created_at = datetime.now()
        self.last_activity = datetime.now()
        self.documents = []
        self.processing_status = 'idle'
        self.chat_history = []
        self.rag_pipeline = None
        self.lock = threading.Lock()
    
    def add_document(self, file_path: str, metadata: Dict[str, Any] = None):
        """Add document to session"""
        with self.lock:
            doc_info = {
                'path': file_path,
                'name': Path(file_path).name,
                'added_at': datetime.now(),
                'processed': False,
                'metadata': metadata or {}
            }
            self.documents.append(doc_info)
            self.last_activity = datetime.now()
    
    def update_activity(self):
        """Update last activity timestamp"""
        self.last_activity = datetime.now()
    
    def is_expired(self, max_age_hours: int = 24) -> bool:
        """Check if session is expired"""
        return datetime.now() - self.last_activity > timedelta(hours=max_age_hours)

class SessionManager:
    """Global session management"""
    
    def __init__(self):
        self.sessions: Dict[str, WebSession] = {}
        self.cleanup_thread = threading.Thread(target=self._cleanup_expired_sessions, daemon=True)
        self.cleanup_thread.start()
    
    def create_session(self) -> str:
        """Create new session"""
        session_id = str(uuid.uuid4())
        self.sessions[session_id] = WebSession(session_id)
        return session_id
    
    def get_session(self, session_id: str) -> Optional[WebSession]:
        """Get session by ID"""
        session = self.sessions.get(session_id)
        if session:
            session.update_activity()
        return session
    
    def _cleanup_expired_sessions(self):
        """Background cleanup of expired sessions"""
        while True:
            expired_sessions = []
            for session_id, session in self.sessions.items():
                if session.is_expired():
                    expired_sessions.append(session_id)
            
            for session_id in expired_sessions:
                del self.sessions[session_id]
            
            time.sleep(3600)  # Run cleanup every hour

# Global session manager instance
session_manager = SessionManager()
```

#### 3.1.3 Integration Points
- **DocumentLoader Integration**: Direct reuse of existing component
- **File Upload Handling**: Secure file storage with session isolation
- **Progress Tracking**: Adapt existing ProgressManager for web updates

#### 3.1.4 Phase 1 Testing
**File: `tests/test_web_ui_phase1.py`**
```python
import pytest
import tempfile
from pathlib import Path
import gradio as gr
from src.web_ui import DocuChatWebUI
from src.web_session import SessionManager

class TestWebUIPhase1:
    def test_session_creation(self):
        """Test session management"""
        ui = DocuChatWebUI()
        session_id = ui.create_session()
        assert session_id in ui.sessions
        assert 'created_at' in ui.sessions[session_id]
    
    def test_file_upload(self):
        """Test document upload functionality"""
        # Implementation for testing file uploads
        pass
    
    def test_document_validation(self):
        """Test supported file type validation"""
        # Implementation for testing file validation
        pass
```

---

### Phase 2: RAG Pipeline Integration (Weeks 3-4)

#### 3.2.1 Objectives
- Integrate complete RAG pipeline with web interface
- Implement real-time processing progress updates
- Create interactive chat interface
- Add session-based vector database management

#### 3.2.2 Enhanced Web Interface

**File: `src/web_rag.py`**
```python
"""
RAG Pipeline integration for web interface.
Handles document processing and question-answering with real-time updates.
"""

import asyncio
import threading
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import time
import json

from embeddings import EmbeddingGenerator
from vector_db import VectorDatabase  
from llm_client import LLMClient
from rag_pipeline import RAGPipeline
from chunker import DocumentChunker
from progress_manager import create_progress_manager

class WebRAGPipeline:
    """Web-enabled RAG Pipeline with session support"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.collection_name = f"docuchat_{session_id}"
        self.pipeline = None
        self.processing_lock = threading.Lock()
        self.progress_callback = None
        
        # Initialize components
        self.embedding_generator = EmbeddingGenerator(
            model_name="all-MiniLM-L6-v2",
            batch_size=32,
            verbose=False
        )
        
        self.vector_database = VectorDatabase(
            persist_directory="./chroma",
            collection_name=self.collection_name,
            verbose=False,
            rebuild=False
        )
        
        self.llm_client = LLMClient(
            base_url="http://localhost:11434",
            model="gemma3:270m",
            timeout=60.0,
            verbose=False
        )
        
        self.chunker = DocumentChunker(
            chunk_size=2000,
            chunk_overlap=200,
            verbose=False
        )
    
    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback
    
    def process_documents_async(self, file_paths: List[str]) -> Dict[str, Any]:
        """Process documents asynchronously with progress updates"""
        def processing_task():
            try:
                with self.processing_lock:
                    return self._process_documents_internal(file_paths)
            except Exception as e:
                if self.progress_callback:
                    self.progress_callback(f"Error: {str(e)}", 'error')
                return {'success': False, 'error': str(e)}
        
        # Run in background thread
        thread = threading.Thread(target=processing_task)
        thread.start()
        return {'success': True, 'message': 'Processing started'}
    
    def _process_documents_internal(self, file_paths: List[str]) -> Dict[str, Any]:
        """Internal document processing logic"""
        results = {
            'processed_files': 0,
            'total_chunks': 0,
            'total_embeddings': 0,
            'processing_time': 0,
            'errors': []
        }
        
        start_time = time.time()
        
        for i, file_path in enumerate(file_paths):
            if self.progress_callback:
                progress = (i / len(file_paths)) * 100
                self.progress_callback(
                    f"Processing {Path(file_path).name}... ({i+1}/{len(file_paths)})",
                    'processing',
                    progress
                )
            
            try:
                # Load document
                from document_loader import DocumentLoader
                loader = DocumentLoader(verbose=False)
                result = loader.load_document(file_path)
                
                if result is None:
                    results['errors'].append(f"Failed to load {file_path}")
                    continue
                
                filename, content = result
                
                # Chunk document
                chunks = list(self.chunker.chunk_document(
                    text=content,
                    source_file=filename,
                    metadata={'processed_at': time.time()}
                ))
                
                if not chunks:
                    results['errors'].append(f"No chunks created for {filename}")
                    continue
                
                # Generate embeddings
                embedded_chunks = self.embedding_generator.embed_chunks(chunks)
                
                if not embedded_chunks:
                    results['errors'].append(f"No embeddings generated for {filename}")
                    continue
                
                # Store in vector database
                success = self.vector_database.add_chunks(embedded_chunks)
                
                if success:
                    results['processed_files'] += 1
                    results['total_chunks'] += len(chunks)
                    results['total_embeddings'] += len(embedded_chunks)
                else:
                    results['errors'].append(f"Failed to store chunks for {filename}")
                
            except Exception as e:
                results['errors'].append(f"Error processing {file_path}: {str(e)}")
        
        results['processing_time'] = time.time() - start_time
        
        # Initialize RAG pipeline if processing was successful
        if results['processed_files'] > 0:
            self.pipeline = RAGPipeline(
                embedding_generator=self.embedding_generator,
                vector_database=self.vector_database,
                llm_client=self.llm_client,
                verbose=False
            )
            
            if self.progress_callback:
                self.progress_callback("Processing completed successfully!", 'complete', 100)
        
        return results
    
    def answer_question(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """Answer question using RAG pipeline"""
        if not self.pipeline:
            return {
                'success': False,
                'error': 'No documents processed yet. Please upload and process documents first.'
            }
        
        try:
            result = self.pipeline.answer_question(
                question=question,
                top_k=top_k,
                include_sources=True
            )
            
            return {
                'success': True,
                'answer': result.answer,
                'sources': result.sources,
                'processing_time': result.processing_time,
                'confidence': result.confidence_score
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Failed to answer question: {str(e)}'
            }
    
    def get_database_info(self) -> Dict[str, Any]:
        """Get database statistics"""
        if self.vector_database:
            return self.vector_database.get_info()
        return {}
    
    def close(self):
        """Clean up resources"""
        if self.pipeline:
            self.pipeline.close()
        if self.vector_database:
            self.vector_database.close()
```

#### 3.2.3 Enhanced Gradio Interface

**Updated File: `src/web_ui.py`**
```python
# Enhanced version with chat interface and real-time updates
import gradio as gr
import json
import time
from typing import List, Tuple, Dict, Any
import threading
from pathlib import Path

from web_rag import WebRAGPipeline
from web_session import session_manager

class DocuChatWebInterface:
    def __init__(self):
        self.active_pipelines: Dict[str, WebRAGPipeline] = {}
        self.processing_status: Dict[str, Dict] = {}
    
    def create_session(self) -> str:
        """Create new user session"""
        return session_manager.create_session()
    
    def upload_and_process(self, files, session_id: str, progress=gr.Progress()) -> Tuple[str, str]:
        """Upload files and start processing"""
        if not files:
            return "No files uploaded", "idle"
        
        session = session_manager.get_session(session_id)
        if not session:
            return "Session expired", "error"
        
        # Initialize RAG pipeline for session
        if session_id not in self.active_pipelines:
            pipeline = WebRAGPipeline(session_id)
            self.active_pipelines[session_id] = pipeline
        
        pipeline = self.active_pipelines[session_id]
        
        # Save uploaded files
        file_paths = []
        upload_dir = Path(f"./uploads/{session_id}")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        for file in files:
            if file is not None:
                file_path = upload_dir / file.name
                with open(file_path, 'wb') as f:
                    f.write(file.read())
                file_paths.append(str(file_path))
                session.add_document(str(file_path))
        
        # Set up progress callback
        def progress_callback(message: str, status: str, percentage: float = None):
            self.processing_status[session_id] = {
                'message': message,
                'status': status,
                'percentage': percentage or 0
            }
            if percentage is not None:
                progress(percentage / 100, desc=message)
        
        pipeline.set_progress_callback(progress_callback)
        
        # Start processing
        result = pipeline.process_documents_async(file_paths)
        
        if result['success']:
            return f"Started processing {len(file_paths)} files", "processing"
        else:
            return f"Error: {result.get('error', 'Unknown error')}", "error"
    
    def get_processing_status(self, session_id: str) -> str:
        """Get current processing status"""
        if session_id in self.processing_status:
            status = self.processing_status[session_id]
            return f"{status['message']} ({status['percentage']:.1f}%)"
        return "Ready"
    
    def chat_with_documents(self, message: str, history: List, session_id: str) -> Tuple[str, List]:
        """Handle chat interaction"""
        if session_id not in self.active_pipelines:
            return "", history + [("System", "Please upload and process documents first.")]
        
        pipeline = self.active_pipelines[session_id]
        result = pipeline.answer_question(message, top_k=5)
        
        if result['success']:
            response = result['answer']
            if result.get('sources'):
                source_files = set()
                for source in result['sources']:
                    if 'metadata' in source and 'source_file' in source['metadata']:
                        source_files.add(Path(source['metadata']['source_file']).name)
                
                if source_files:
                    response += f"\n\n*Sources: {', '.join(sorted(source_files))}*"
        else:
            response = f"Error: {result['error']}"
        
        history.append((message, response))
        return "", history

def create_gradio_interface():
    ui = DocuChatWebInterface()
    
    with gr.Blocks(title="DocuChat - Web Interface", theme=gr.themes.Soft()) as app:
        # Session state
        session_id = gr.State(value=ui.create_session)
        
        gr.Markdown("# DocuChat - Intelligent Document Q&A")
        gr.Markdown("Upload documents and ask questions about their content using advanced RAG technology.")
        
        with gr.Tabs():
            # Document Processing Tab
            with gr.Tab("ðŸ“„ Documents"):
                with gr.Row():
                    with gr.Column(scale=2):
                        file_upload = gr.Files(
                            label="Upload Documents",
                            file_types=[".pdf", ".docx", ".txt", ".md"],
                            file_count="multiple",
                            height=200
                        )
                        
                        process_btn = gr.Button("Process Documents", variant="primary", size="lg")
                        
                        with gr.Row():
                            status_display = gr.Textbox(
                                label="Processing Status",
                                value="Ready to process documents",
                                interactive=False
                            )
                            
                    with gr.Column(scale=1):
                        gr.Markdown("### Processing Options")
                        chunk_size = gr.Slider(
                            minimum=500,
                            maximum=4000,
                            value=2000,
                            step=100,
                            label="Chunk Size",
                            info="Size of text chunks for processing"
                        )
                        
                        top_k = gr.Slider(
                            minimum=1,
                            maximum=20,
                            value=5,
                            step=1,
                            label="Top-K Results",
                            info="Number of relevant chunks to retrieve"
                        )
            
            # Chat Interface Tab  
            with gr.Tab("ðŸ’¬ Chat"):
                with gr.Row():
                    with gr.Column(scale=3):
                        chatbot = gr.Chatbot(
                            label="Document Q&A",
                            height=500,
                            show_label=True
                        )
                        
                        with gr.Row():
                            msg = gr.Textbox(
                                placeholder="Ask a question about your documents...",
                                scale=4,
                                container=False
                            )
                            submit_btn = gr.Button("Send", variant="primary", scale=1)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### Quick Actions")
                        clear_btn = gr.Button("Clear Chat", variant="secondary")
                        
                        gr.Markdown("### Session Info")
                        session_info = gr.JSON(
                            label="Current Session",
                            value={"status": "Active", "documents": 0}
                        )
            
            # Database Info Tab
            with gr.Tab("ðŸ“Š Database"):
                db_info = gr.JSON(label="Vector Database Information")
                refresh_db_btn = gr.Button("Refresh Database Info")
        
        # Event handlers
        process_btn.click(
            fn=ui.upload_and_process,
            inputs=[file_upload, session_id],
            outputs=[status_display]
        )
        
        # Auto-refresh status every 2 seconds during processing
        status_display.change(
            fn=ui.get_processing_status,
            inputs=[session_id],
            outputs=[status_display],
            every=2
        )
        
        # Chat handlers
        submit_btn.click(
            fn=ui.chat_with_documents,
            inputs=[msg, chatbot, session_id],
            outputs=[msg, chatbot]
        )
        
        msg.submit(
            fn=ui.chat_with_documents,
            inputs=[msg, chatbot, session_id],
            outputs=[msg, chatbot]
        )
        
        clear_btn.click(lambda: [], outputs=[chatbot])
        
        refresh_db_btn.click(
            fn=lambda sid: ui.active_pipelines.get(sid, {}).get_database_info() if sid in ui.active_pipelines else {},
            inputs=[session_id],
            outputs=[db_info]
        )
    
    return app

if __name__ == "__main__":
    app = create_gradio_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        debug=True
    )
```

---

### Phase 3: Advanced Features & Production Readiness (Weeks 5-6)

#### 3.3.1 Objectives
- Implement advanced UI features (document management, chat history)
- Add user authentication and multi-user support
- Optimize performance for production deployment
- Implement comprehensive error handling and logging

#### 3.3.2 Advanced Features Implementation

**File: `src/web_advanced.py`**
```python
"""
Advanced web UI features for production deployment.
Includes authentication, document management, and admin features.
"""

import os
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import json
import sqlite3
from pathlib import Path

class UserAuthentication:
    """Simple user authentication system"""
    
    def __init__(self, db_path: str = "./users.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """Initialize user database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS users (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    username TEXT UNIQUE NOT NULL,
                    password_hash TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_login TIMESTAMP,
                    is_active BOOLEAN DEFAULT TRUE
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS user_sessions (
                    session_id TEXT PRIMARY KEY,
                    username TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    expires_at TIMESTAMP NOT NULL,
                    FOREIGN KEY (username) REFERENCES users (username)
                )
            """)
    
    def create_user(self, username: str, password: str) -> bool:
        """Create new user account"""
        password_hash = hashlib.sha256(password.encode()).hexdigest()
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    "INSERT INTO users (username, password_hash) VALUES (?, ?)",
                    (username, password_hash)
                )
            return True
        except sqlite3.IntegrityError:
            return False  # Username already exists
    
    def authenticate(self, username: str, password: str) -> Optional[str]:
        """Authenticate user and return session token"""
        password_hash = hashlib.sha256(password.encode()).hexdigest()
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT username FROM users WHERE username = ? AND password_hash = ? AND is_active = TRUE",
                (username, password_hash)
            )
            
            if cursor.fetchone():
                # Create session
                session_token = hashlib.sha256(f"{username}{datetime.now()}".encode()).hexdigest()
                expires_at = datetime.now() + timedelta(hours=24)
                
                conn.execute(
                    "INSERT INTO user_sessions (session_id, username, expires_at) VALUES (?, ?, ?)",
                    (session_token, username, expires_at)
                )
                
                # Update last login
                conn.execute(
                    "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE username = ?",
                    (username,)
                )
                
                return session_token
        
        return None
    
    def validate_session(self, session_token: str) -> Optional[str]:
        """Validate session token and return username"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT username FROM user_sessions 
                WHERE session_id = ? AND expires_at > CURRENT_TIMESTAMP
            """, (session_token,))
            
            result = cursor.fetchone()
            return result[0] if result else None

class DocumentManager:
    """Advanced document management with metadata"""
    
    def __init__(self, storage_path: str = "./document_storage"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        self.db_path = self.storage_path / "documents.db"
        self._init_database()
    
    def _init_database(self):
        """Initialize document metadata database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    original_name TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    file_size INTEGER NOT NULL,
                    mime_type TEXT,
                    upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id TEXT NOT NULL,
                    username TEXT,
                    processing_status TEXT DEFAULT 'pending',
                    chunk_count INTEGER DEFAULT 0,
                    embedding_count INTEGER DEFAULT 0,
                    metadata TEXT  -- JSON metadata
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS chat_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    username TEXT,
                    question TEXT NOT NULL,
                    answer TEXT NOT NULL,
                    sources TEXT,  -- JSON list of sources
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    processing_time REAL,
                    confidence_score REAL
                )
            """)
    
    def save_document(self, file_data: bytes, filename: str, session_id: str, 
                     username: Optional[str] = None) -> str:
        """Save uploaded document with metadata"""
        # Generate unique filename
        file_hash = hashlib.md5(file_data).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        stored_filename = f"{timestamp}_{file_hash}_{filename}"
        
        # Save file
        file_path = self.storage_path / stored_filename
        with open(file_path, 'wb') as f:
            f.write(file_data)
        
        # Save metadata
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO documents 
                (filename, original_name, file_path, file_size, session_id, username)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (stored_filename, filename, str(file_path), len(file_data), session_id, username))
        
        return str(file_path)
    
    def get_session_documents(self, session_id: str) -> List[Dict[str, Any]]:
        """Get all documents for a session"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT id, filename, original_name, file_size, upload_time, 
                       processing_status, chunk_count, embedding_count
                FROM documents WHERE session_id = ?
                ORDER BY upload_time DESC
            """, (session_id,))
            
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def update_processing_status(self, file_path: str, status: str, 
                               chunk_count: int = 0, embedding_count: int = 0):
        """Update document processing status"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                UPDATE documents 
                SET processing_status = ?, chunk_count = ?, embedding_count = ?
                WHERE file_path = ?
            """, (status, chunk_count, embedding_count, file_path))
    
    def save_chat_interaction(self, session_id: str, username: Optional[str],
                            question: str, answer: str, sources: List[Dict],
                            processing_time: float, confidence: float):
        """Save chat interaction to history"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO chat_history 
                (session_id, username, question, answer, sources, processing_time, confidence_score)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (session_id, username, question, answer, json.dumps(sources), 
                  processing_time, confidence))
    
    def get_chat_history(self, session_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        """Get chat history for session"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT question, answer, sources, timestamp, processing_time, confidence_score
                FROM chat_history 
                WHERE session_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
            """, (session_id, limit))
            
            columns = [desc[0] for desc in cursor.description]
            history = []
            for row in cursor.fetchall():
                item = dict(zip(columns, row))
                if item['sources']:
                    item['sources'] = json.loads(item['sources'])
                history.append(item)
            
            return list(reversed(history))  # Return in chronological order

# Production Configuration
class ProductionConfig:
    """Production deployment configuration"""
    
    # Security settings
    ENABLE_AUTH = os.getenv('DOCUCHAT_ENABLE_AUTH', 'false').lower() == 'true'
    SECRET_KEY = os.getenv('DOCUCHAT_SECRET_KEY', 'dev-secret-key-change-in-production')
    
    # Performance settings
    MAX_FILE_SIZE = int(os.getenv('DOCUCHAT_MAX_FILE_SIZE', '50')) * 1024 * 1024  # 50MB default
    MAX_FILES_PER_UPLOAD = int(os.getenv('DOCUCHAT_MAX_FILES', '10'))
    SESSION_TIMEOUT_HOURS = int(os.getenv('DOCUCHAT_SESSION_TIMEOUT', '24'))
    
    # Storage settings
    UPLOAD_PATH = os.getenv('DOCUCHAT_UPLOAD_PATH', './uploads')
    DATABASE_PATH = os.getenv('DOCUCHAT_DB_PATH', './chroma')
    
    # Ollama settings
    OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
    OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gemma3:270m')
    OLLAMA_TIMEOUT = float(os.getenv('OLLAMA_TIMEOUT', '60.0'))
    
    # UI settings
    SERVER_HOST = os.getenv('DOCUCHAT_HOST', '0.0.0.0')
    SERVER_PORT = int(os.getenv('DOCUCHAT_PORT', '7860'))
    ENABLE_SHARING = os.getenv('DOCUCHAT_ENABLE_SHARING', 'false').lower() == 'true'
    
    @classmethod
    def validate(cls) -> List[str]:
        """Validate configuration and return any issues"""
        issues = []
        
        if cls.ENABLE_AUTH and cls.SECRET_KEY == 'dev-secret-key-change-in-production':
            issues.append("Production secret key must be changed when authentication is enabled")
        
        if not Path(cls.UPLOAD_PATH).exists():
            try:
                Path(cls.UPLOAD_PATH).mkdir(parents=True, exist_ok=True)
            except Exception as e:
                issues.append(f"Cannot create upload directory: {e}")
        
        # Test Ollama connection
        try:
            import requests
            response = requests.get(f"{cls.OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code != 200:
                issues.append("Ollama service not accessible")
        except Exception:
            issues.append("Cannot connect to Ollama service")
        
        return issues
```

---

## 4. Technical Integration Points

### 4.1 Existing Component Integration

#### 4.1.1 DocumentLoader Integration
```python
# Integration pattern for document loading
from document_loader import DocumentLoader

class WebDocumentProcessor:
    def __init__(self):
        self.loader = DocumentLoader(verbose=False)
    
    async def process_uploaded_file(self, file_data: bytes, filename: str) -> Dict[str, Any]:
        """Process uploaded file using existing DocumentLoader"""
        # Save temporary file
        temp_path = f"/tmp/{filename}"
        with open(temp_path, 'wb') as f:
            f.write(file_data)
        
        # Use existing loader
        result = self.loader.load_document(temp_path)
        
        # Cleanup
        os.unlink(temp_path)
        
        if result:
            filename, content = result
            return {
                'success': True,
                'filename': filename,
                'content': content,
                'character_count': len(content)
            }
        
        return {'success': False, 'error': 'Failed to process document'}
```

#### 4.1.2 RAG Pipeline Integration
```python
# Direct integration with existing RAG pipeline
from rag_pipeline import RAGPipeline
from embeddings import EmbeddingGenerator
from vector_db import VectorDatabase
from llm_client import LLMClient

class WebRAGService:
    def __init__(self, session_id: str):
        # Initialize existing components with session-specific settings
        self.embedding_generator = EmbeddingGenerator(
            model_name="all-MiniLM-L6-v2",
            batch_size=32,
            verbose=False
        )
        
        self.vector_database = VectorDatabase(
            persist_directory="./chroma",
            collection_name=f"docuchat_{session_id}",
            verbose=False
        )
        
        self.llm_client = LLMClient(
            base_url="http://localhost:11434",
            model="gemma3:270m",
            timeout=60.0
        )
        
        self.pipeline = RAGPipeline(
            embedding_generator=self.embedding_generator,
            vector_database=self.vector_database,
            llm_client=self.llm_client
        )
    
    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """Query using existing pipeline"""
        try:
            result = self.pipeline.answer_question(question, top_k, include_sources=True)
            return {
                'success': True,
                'answer': result.answer,
                'sources': result.sources,
                'processing_time': result.processing_time,
                'confidence': result.confidence_score
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}
```

### 4.2 Database Integration Patterns

#### 4.2.1 ChromaDB Session Isolation
```python
# Session-based collection management
class SessionVectorDB:
    def __init__(self, base_path: str = "./chroma"):
        self.base_path = Path(base_path)
        self.active_sessions = {}
    
    def get_session_db(self, session_id: str) -> VectorDatabase:
        """Get or create session-specific database"""
        if session_id not in self.active_sessions:
            self.active_sessions[session_id] = VectorDatabase(
                persist_directory=str(self.base_path),
                collection_name=f"session_{session_id}",
                verbose=False
            )
        return self.active_sessions[session_id]
    
    def cleanup_session(self, session_id: str):
        """Clean up session resources"""
        if session_id in self.active_sessions:
            self.active_sessions[session_id].close()
            del self.active_sessions[session_id]
```

### 4.3 Ollama Integration Patterns

#### 4.3.1 Connection Management
```python
# Robust Ollama connection handling
class WebLLMClient:
    def __init__(self, base_url: str, model: str):
        self.client = LLMClient(base_url=base_url, model=model, timeout=60.0)
        self.connection_pool = {}
    
    async def health_check(self) -> bool:
        """Check Ollama service health"""
        try:
            return self.client.is_available()
        except Exception:
            return False
    
    async def generate_with_retry(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """Generate with retry logic"""
        for attempt in range(max_retries):
            try:
                response = self.client.generate(prompt)
                return response.response
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
        return None
```

---

## 5. UI/UX Design Specifications

### 5.1 Design Principles

#### 5.1.1 Visual Hierarchy
- **Primary Actions**: Document upload and chat input (prominent positioning)
- **Secondary Actions**: Processing options and settings (accessible but not dominant)
- **Tertiary Actions**: Database info and admin features (utility placement)

#### 5.1.2 User Flow Design
```
User Entry â†’ Document Upload â†’ Processing Feedback â†’ Chat Interface
     â†“              â†“                 â†“                   â†“
 Welcome Screen â†’ File Selection â†’ Progress Display â†’ Q&A Interaction
     â†“              â†“                 â†“                   â†“
 Quick Start â†’ Validation Check â†’ Real-time Updates â†’ Source Attribution
```

### 5.2 Interface Components

#### 5.2.1 Document Upload Interface
```python
# Gradio component configuration
file_upload = gr.Files(
    label="ðŸ“„ Upload Documents",
    file_types=[".pdf", ".docx", ".txt", ".md"],
    file_count="multiple",
    height=200,
    show_label=True,
    container=True,
    scale=2
)

# Styling with custom CSS
css = """
.file-upload-area {
    border: 2px dashed #e1e5e9;
    border-radius: 8px;
    padding: 20px;
    text-align: center;
    transition: border-color 0.3s ease;
}

.file-upload-area:hover {
    border-color: #007bff;
    background-color: #f8f9fa;
}

.processing-status {
    background: linear-gradient(90deg, #007bff 0%, #0056b3 100%);
    color: white;
    padding: 10px;
    border-radius: 5px;
    margin: 10px 0;
}
"""
```

#### 5.2.2 Chat Interface Design
```python
# Chat interface with enhanced UX
chatbot = gr.Chatbot(
    label="ðŸ’¬ Document Q&A Assistant",
    height=500,
    show_label=True,
    avatar_images=("ðŸ‘¤", "ðŸ¤–"),
    bubble_full_width=False,
    show_share_button=False,
    layout="panel"
)

# Message input with suggestions
with gr.Row():
    msg = gr.Textbox(
        placeholder="Ask about your documents... (e.g., 'What are the main topics?')",
        scale=4,
        container=False,
        show_label=False
    )
    
    submit_btn = gr.Button(
        "Send",
        variant="primary",
        scale=1,
        size="lg"
    )

# Quick suggestion buttons
suggestion_buttons = gr.Row([
    gr.Button("ðŸ“‹ Summarize", size="sm", variant="outline"),
    gr.Button("ðŸ” Key Points", size="sm", variant="outline"),
    gr.Button("â“ Ask Question", size="sm", variant="outline")
])
```

### 5.3 Responsive Design Patterns

#### 5.3.1 Mobile-First Layout
```python
# Responsive column layout
with gr.Blocks() as app:
    # Mobile-optimized stack layout
    with gr.Column(scale=1):
        # Full-width components on mobile
        file_upload = gr.Files(...)
        
        # Collapsible advanced options
        with gr.Accordion("Advanced Options", open=False):
            chunk_size = gr.Slider(...)
            top_k = gr.Slider(...)
    
    # Desktop: side-by-side layout
    with gr.Row(visible=gr.utils.get_space() == "desktop"):
        with gr.Column(scale=2):
            # Main content
            pass
        with gr.Column(scale=1):
            # Sidebar content
            pass
```

### 5.4 Accessibility Features

#### 5.4.1 Screen Reader Support
```python
# Accessible component configuration
file_upload = gr.Files(
    label="Upload Documents",
    file_types=[".pdf", ".docx", ".txt", ".md"],
    elem_id="document-upload",
    elem_classes=["accessible-upload"]
)

# ARIA labels and descriptions
chatbot = gr.Chatbot(
    label="Document Question and Answer Chat",
    elem_id="main-chat",
    info="Chat interface for asking questions about uploaded documents"
)
```

#### 5.4.2 Keyboard Navigation
```python
# Keyboard shortcuts configuration
app.load(
    fn=None,
    js="""
    function() {
        // Ctrl+Enter to send message
        document.addEventListener('keydown', function(e) {
            if (e.ctrlKey && e.key === 'Enter') {
                document.querySelector('#send-button').click();
            }
        });
        
        // Escape to clear chat
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                document.querySelector('#clear-button').click();
            }
        });
    }
    """
)
```

---

## 6. Deployment and Maintenance

### 6.1 Deployment Architecture

#### 6.1.1 Development Deployment
```bash
# Local development setup
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Start Ollama service
ollama serve &
ollama pull gemma3:270m

# Launch web interface
python src/web_ui.py
```

#### 6.1.2 Production Deployment
```dockerfile
# Dockerfile for production deployment
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p /app/uploads /app/chroma /app/logs

# Set environment variables
ENV DOCUCHAT_HOST=0.0.0.0
ENV DOCUCHAT_PORT=7860
ENV DOCUCHAT_ENABLE_AUTH=true

EXPOSE 7860

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:7860/health || exit 1

# Run application
CMD ["python", "src/web_ui.py"]
```

#### 6.1.3 Docker Compose Configuration
```yaml
# docker-compose.yml
version: '3.8'

services:
  docuchat-web:
    build: .
    ports:
      - "7860:7860"
    volumes:
      - ./uploads:/app/uploads
      - ./chroma:/app/chroma
      - ./logs:/app/logs
    environment:
      - DOCUCHAT_ENABLE_AUTH=true
      - DOCUCHAT_SECRET_KEY=${SECRET_KEY}
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  ollama_data:
```

### 6.2 Configuration Management

#### 6.2.1 Environment Configuration
```python
# config.py - Centralized configuration management
import os
from pathlib import Path
from typing import Optional

class Config:
    """Application configuration management"""
    
    # Basic settings
    DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'
    HOST = os.getenv('DOCUCHAT_HOST', '0.0.0.0')
    PORT = int(os.getenv('DOCUCHAT_PORT', '7860'))
    
    # Security
    SECRET_KEY = os.getenv('DOCUCHAT_SECRET_KEY', 'dev-key-change-in-production')
    ENABLE_AUTH = os.getenv('DOCUCHAT_ENABLE_AUTH', 'false').lower() == 'true'
    
    # File handling
    MAX_FILE_SIZE = int(os.getenv('MAX_FILE_SIZE', '50')) * 1024 * 1024  # MB
    MAX_FILES_PER_UPLOAD = int(os.getenv('MAX_FILES_PER_UPLOAD', '10'))
    UPLOAD_PATH = Path(os.getenv('UPLOAD_PATH', './uploads'))
    
    # RAG settings
    CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '2000'))
    CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', '200'))
    DEFAULT_TOP_K = int(os.getenv('DEFAULT_TOP_K', '5'))
    
    # Ollama configuration
    OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
    OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gemma3:270m')
    OLLAMA_TIMEOUT = float(os.getenv('OLLAMA_TIMEOUT', '60.0'))
    
    # Database
    CHROMA_PATH = Path(os.getenv('CHROMA_PATH', './chroma'))
    
    # Logging
    LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
    LOG_PATH = Path(os.getenv('LOG_PATH', './logs'))
    
    @classmethod
    def validate(cls) -> list[str]:
        """Validate configuration"""
        errors = []
        
        if cls.ENABLE_AUTH and cls.SECRET_KEY == 'dev-key-change-in-production':
            errors.append("SECRET_KEY must be changed in production")
        
        if not cls.UPLOAD_PATH.exists():
            cls.UPLOAD_PATH.mkdir(parents=True, exist_ok=True)
        
        if not cls.CHROMA_PATH.exists():
            cls.CHROMA_PATH.mkdir(parents=True, exist_ok=True)
        
        if not cls.LOG_PATH.exists():
            cls.LOG_PATH.mkdir(parents=True, exist_ok=True)
        
        return errors
```

### 6.3 Monitoring and Logging

#### 6.3.1 Application Logging
```python
# logging_config.py
import logging
import sys
from pathlib import Path
from logging.handlers import RotatingFileHandler

def setup_logging(log_level: str = "INFO", log_path: Path = Path("./logs")):
    """Configure application logging"""
    
    # Ensure log directory exists
    log_path.mkdir(parents=True, exist_ok=True)
    
    # Configure root logger
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            # Console handler
            logging.StreamHandler(sys.stdout),
            
            # File handler with rotation
            RotatingFileHandler(
                log_path / "docuchat.log",
                maxBytes=10*1024*1024,  # 10MB
                backupCount=5
            ),
            
            # Error file handler
            RotatingFileHandler(
                log_path / "errors.log",
                maxBytes=10*1024*1024,
                backupCount=5
            )
        ]
    )
    
    # Set specific loggers
    logging.getLogger('docuchat').setLevel(logging.DEBUG)
    logging.getLogger('gradio').setLevel(logging.WARNING)
    logging.getLogger('transformers').setLevel(logging.WARNING)
```

#### 6.3.2 Performance Monitoring
```python
# monitoring.py
import time
import psutil
import threading
from datetime import datetime
from typing import Dict, Any
import json

class PerformanceMonitor:
    """System and application performance monitoring"""
    
    def __init__(self):
        self.metrics = {
            'requests_total': 0,
            'requests_successful': 0,
            'requests_failed': 0,
            'processing_times': [],
            'system_metrics': {}
        }
        self.lock = threading.Lock()
    
    def record_request(self, processing_time: float, success: bool):
        """Record request metrics"""
        with self.lock:
            self.metrics['requests_total'] += 1
            if success:
                self.metrics['requests_successful'] += 1
            else:
                self.metrics['requests_failed'] += 1
            
            self.metrics['processing_times'].append(processing_time)
            
            # Keep only last 1000 processing times
            if len(self.metrics['processing_times']) > 1000:
                self.metrics['processing_times'] = self.metrics['processing_times'][-1000:]
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """Get current system metrics"""
        return {
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'timestamp': datetime.now().isoformat()
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary"""
        with self.lock:
            processing_times = self.metrics['processing_times']
            
            summary = {
                'requests': {
                    'total': self.metrics['requests_total'],
                    'successful': self.metrics['requests_successful'],
                    'failed': self.metrics['requests_failed'],
                    'success_rate': (
                        self.metrics['requests_successful'] / max(1, self.metrics['requests_total'])
                    ) * 100
                },
                'performance': {
                    'avg_processing_time': sum(processing_times) / len(processing_times) if processing_times else 0,
                    'min_processing_time': min(processing_times) if processing_times else 0,
                    'max_processing_time': max(processing_times) if processing_times else 0
                },
                'system': self.get_system_metrics()
            }
            
            return summary

# Global monitor instance
performance_monitor = PerformanceMonitor()
```

### 6.4 Backup and Recovery

#### 6.4.1 Database Backup
```python
# backup.py
import shutil
import sqlite3
from pathlib import Path
from datetime import datetime
import tarfile

class BackupManager:
    """Backup and recovery management"""
    
    def __init__(self, backup_path: Path = Path("./backups")):
        self.backup_path = backup_path
        self.backup_path.mkdir(parents=True, exist_ok=True)
    
    def create_full_backup(self, data_paths: list[Path]) -> str:
        """Create full system backup"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.backup_path / f"docuchat_backup_{timestamp}.tar.gz"
        
        with tarfile.open(backup_file, "w:gz") as tar:
            for path in data_paths:
                if path.exists():
                    tar.add(path, arcname=path.name)
        
        return str(backup_file)
    
    def restore_backup(self, backup_file: str, restore_path: Path):
        """Restore from backup"""
        with tarfile.open(backup_file, "r:gz") as tar:
            tar.extractall(path=restore_path)
    
    def cleanup_old_backups(self, keep_days: int = 30):
        """Remove backups older than specified days"""
        cutoff = datetime.now().timestamp() - (keep_days * 24 * 3600)
        
        for backup_file in self.backup_path.glob("docuchat_backup_*.tar.gz"):
            if backup_file.stat().st_mtime < cutoff:
                backup_file.unlink()
```

---

## 7. Risk Assessment and Mitigation

### 7.1 Technical Risks

#### 7.1.1 Performance Risks
**Risk**: Web interface performance degradation with large documents
- **Impact**: High - Poor user experience, timeouts
- **Probability**: Medium - Likely with large document sets
- **Mitigation Strategy**:
  ```python
  # Implement streaming and chunked processing
  async def process_documents_streaming(files: list) -> AsyncGenerator:
      for file in files:
          # Process one file at a time with yield
          result = await process_single_file(file)
          yield result
  
  # Add progress indicators and cancellation
  def cancellable_processing(session_id: str):
      if session_id in cancellation_flags and cancellation_flags[session_id]:
          raise ProcessingCancelledException()
  ```

#### 7.1.2 Memory Usage Risks  
**Risk**: High memory consumption with concurrent users
- **Impact**: High - System crashes, service unavailability
- **Probability**: Medium - Dependent on usage patterns
- **Mitigation Strategy**:
  ```python
  # Resource limits and monitoring
  class ResourceManager:
      def __init__(self, max_memory_mb: int = 2048):
          self.max_memory = max_memory_mb * 1024 * 1024
          self.active_sessions = {}
      
      def check_memory_usage(self) -> bool:
          current_usage = psutil.Process().memory_info().rss
          return current_usage < self.max_memory
      
      def limit_concurrent_processing(self, max_concurrent: int = 3):
          active_count = sum(1 for s in self.active_sessions.values() if s.is_processing)
          return active_count < max_concurrent
  ```

### 7.2 Security Risks

#### 7.2.1 File Upload Security
**Risk**: Malicious file uploads, path traversal attacks
- **Impact**: High - System compromise, data breach
- **Probability**: Medium - Common attack vector
- **Mitigation Strategy**:
  ```python
  # Secure file handling
  def validate_upload(file_data: bytes, filename: str) -> bool:
      # File type validation
      if not any(filename.lower().endswith(ext) for ext in ['.pdf', '.docx', '.txt']):
          return False
      
      # Size validation
      if len(file_data) > MAX_FILE_SIZE:
          return False
      
      # Content validation
      try:
          # Attempt to parse with appropriate library
          if filename.lower().endswith('.pdf'):
              import pypdf
              pypdf.PdfReader(io.BytesIO(file_data))
      except Exception:
          return False
      
      return True
  
  def secure_filename(filename: str) -> str:
      # Remove dangerous characters and path components
      import re
      filename = re.sub(r'[^\w\-_\.]', '', filename)
      return f"{uuid.uuid4()}_{filename}"
  ```

#### 7.2.2 Injection Attack Risks
**Risk**: Prompt injection, XSS attacks through chat interface
- **Impact**: Medium - Information disclosure, system manipulation
- **Probability**: Medium - Increasingly common with LLM systems
- **Mitigation Strategy**:
  ```python
  # Input sanitization and validation
  def sanitize_user_input(text: str) -> str:
      # Remove potential injection patterns
      dangerous_patterns = [
          r'<script.*?</script>',
          r'javascript:',
          r'data:',
          r'<iframe',
          r'system',
          r'ignore previous',
          r'new instructions'
      ]
      
      for pattern in dangerous_patterns:
          text = re.sub(pattern, '[FILTERED]', text, flags=re.IGNORECASE)
      
      # Length limiting
      if len(text) > 10000:
          text = text[:10000] + "[TRUNCATED]"
      
      return html.escape(text)
  
  # Rate limiting
  class RateLimiter:
      def __init__(self, max_requests: int = 60, time_window: int = 60):
          self.max_requests = max_requests
          self.time_window = time_window
          self.requests = {}
      
      def check_rate_limit(self, session_id: str) -> bool:
          now = time.time()
          if session_id not in self.requests:
              self.requests[session_id] = []
          
          # Remove old requests
          self.requests[session_id] = [
              req_time for req_time in self.requests[session_id]
              if now - req_time < self.time_window
          ]
          
          # Check limit
          if len(self.requests[session_id]) >= self.max_requests:
              return False
          
          self.requests[session_id].append(now)
          return True
  ```

### 7.3 Availability Risks

#### 7.3.1 Ollama Service Dependency
**Risk**: Ollama service unavailability or crashes
- **Impact**: High - Core functionality unavailable
- **Probability**: Medium - External service dependency
- **Mitigation Strategy**:
  ```python
  # Health checking and automatic recovery
  class OllamaHealthChecker:
      def __init__(self, check_interval: int = 30):
          self.check_interval = check_interval
          self.is_healthy = False
          self.last_check = 0
          
      async def health_check(self) -> bool:
          try:
              response = await asyncio.wait_for(
                  aiohttp.ClientSession().get(f"{OLLAMA_BASE_URL}/api/tags"),
                  timeout=5
              )
              self.is_healthy = response.status == 200
              return self.is_healthy
          except Exception:
              self.is_healthy = False
              return False
      
      async def start_health_monitoring(self):
          while True:
              await self.health_check()
              await asyncio.sleep(self.check_interval)
  
  # Circuit breaker pattern
  class CircuitBreaker:
      def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
          self.failure_threshold = failure_threshold
          self.recovery_timeout = recovery_timeout
          self.failure_count = 0
          self.last_failure_time = None
          self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
      
      async def call(self, func, *args, **kwargs):
          if self.state == "OPEN":
              if time.time() - self.last_failure_time > self.recovery_timeout:
                  self.state = "HALF_OPEN"
              else:
                  raise ServiceUnavailableError("Circuit breaker is OPEN")
          
          try:
              result = await func(*args, **kwargs)
              if self.state == "HALF_OPEN":
                  self.state = "CLOSED"
                  self.failure_count = 0
              return result
          except Exception as e:
              self.failure_count += 1
              self.last_failure_time = time.time()
              
              if self.failure_count >= self.failure_threshold:
                  self.state = "OPEN"
              
              raise e
  ```

### 7.4 Data Protection Risks

#### 7.4.1 Document Privacy and Retention
**Risk**: Unauthorized access to user documents, data retention violations
- **Impact**: High - Privacy breach, regulatory violations
- **Probability**: Medium - Depends on deployment environment
- **Mitigation Strategy**:
  ```python
  # Document encryption and secure storage
  import cryptography.fernet
  
  class SecureDocumentStorage:
      def __init__(self, encryption_key: bytes):
          self.cipher = Fernet(encryption_key)
      
      def store_document(self, content: bytes, metadata: dict) -> str:
          # Encrypt content
          encrypted_content = self.cipher.encrypt(content)
          
          # Store with metadata
          doc_id = str(uuid.uuid4())
          storage_path = SECURE_STORAGE_PATH / f"{doc_id}.enc"
          
          with open(storage_path, 'wb') as f:
              f.write(encrypted_content)
          
          # Store metadata separately
          self.store_metadata(doc_id, metadata)
          return doc_id
      
      def retrieve_document(self, doc_id: str) -> bytes:
          storage_path = SECURE_STORAGE_PATH / f"{doc_id}.enc"
          
          with open(storage_path, 'rb') as f:
              encrypted_content = f.read()
          
          return self.cipher.decrypt(encrypted_content)
      
      def schedule_deletion(self, doc_id: str, retention_days: int = 30):
          # Schedule automatic deletion
          deletion_time = datetime.now() + timedelta(days=retention_days)
          self.deletion_scheduler.add_job(
              self.delete_document,
              'date',
              run_date=deletion_time,
              args=[doc_id]
          )
  
  # GDPR compliance utilities
  class GDPRCompliance:
      def __init__(self):
          self.data_registry = {}
      
      def register_personal_data(self, session_id: str, data_type: str, purpose: str):
          if session_id not in self.data_registry:
              self.data_registry[session_id] = []
          
          self.data_registry[session_id].append({
              'data_type': data_type,
              'purpose': purpose,
              'created_at': datetime.now(),
              'retention_period': timedelta(days=30)
          })
      
      def export_user_data(self, session_id: str) -> dict:
          # Provide data export for GDPR compliance
          user_data = {
              'session_id': session_id,
              'documents': self.get_user_documents(session_id),
              'chat_history': self.get_user_chat_history(session_id),
              'processing_logs': self.get_user_logs(session_id)
          }
          return user_data
      
      def delete_user_data(self, session_id: str):
          # Complete user data deletion
          self.delete_user_documents(session_id)
          self.delete_user_chat_history(session_id)
          self.delete_user_embeddings(session_id)
          del self.data_registry[session_id]
  ```

---

## 8. Development Guidelines

### 8.1 Code Organization

#### 8.1.1 Project Structure
```
DocuChat/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ui.py              # Main Gradio interface
â”‚   â”‚   â”œâ”€â”€ rag_service.py     # Web RAG service layer
â”‚   â”‚   â”œâ”€â”€ session.py         # Session management
â”‚   â”‚   â”œâ”€â”€ auth.py           # Authentication (optional)
â”‚   â”‚   â””â”€â”€ monitoring.py     # Performance monitoring
â”‚   â”œâ”€â”€ core/                 # Existing RAG components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ vector_db.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â””â”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ logging_config.py
â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â””â”€â”€ validators.py
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_web_ui.py
â”‚       â”œâ”€â”€ test_rag_service.py
â”‚       â””â”€â”€ test_integration.py
â”œâ”€â”€ static/                   # Static assets (CSS, JS, images)
â”œâ”€â”€ templates/               # Custom Gradio templates (if needed)
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ docker-compose.prod.yml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â””â”€â”€ app.py                   # Main application entry point
```

#### 8.1.2 Import Standards
```python
# Standard library imports
import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta

# Third-party imports
import gradio as gr
import numpy as np
import requests
from sentence_transformers import SentenceTransformer

# Local imports
from src.core.rag_pipeline import RAGPipeline
from src.web.session import SessionManager
from src.utils.config import Config
from src.utils.security import sanitize_input
```

### 8.2 Coding Standards

#### 8.2.1 Function Documentation
```python
def process_document_upload(
    files: List[gr.File],
    session_id: str,
    chunk_size: int = 2000,
    progress_callback: Optional[callable] = None
) -> Tuple[bool, str, Dict[str, Any]]:
    """
    Process uploaded documents through the RAG pipeline.
    
    Args:
        files: List of uploaded files from Gradio interface
        session_id: Unique session identifier for isolation
        chunk_size: Size of text chunks for processing (default: 2000)
        progress_callback: Optional callback for progress updates
        
    Returns:
        Tuple containing:
        - success: Boolean indicating processing success
        - message: Human-readable status message
        - metadata: Dictionary with processing statistics
        
    Raises:
        ValueError: If files list is empty or invalid
        ProcessingError: If document processing fails
        
    Example:
        >>> files = [uploaded_pdf_file]
        >>> success, msg, stats = process_document_upload(files, "session_123")
        >>> print(f"Success: {success}, Message: {msg}")
        Success: True, Message: Processed 1 documents successfully
    """
    if not files:
        raise ValueError("Files list cannot be empty")
    
    # Implementation...
    return success, message, metadata
```

#### 8.2.2 Error Handling Patterns
```python
# Standardized error handling
class DocuChatError(Exception):
    """Base exception for DocuChat application"""
    pass

class DocumentProcessingError(DocuChatError):
    """Raised when document processing fails"""
    pass

class SessionError(DocuChatError):
    """Raised when session operations fail"""
    pass

class LLMServiceError(DocuChatError):
    """Raised when LLM service is unavailable"""
    pass

# Error handling decorator
from functools import wraps
import logging

def handle_errors(error_message: str = "Operation failed"):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except DocuChatError as e:
                logging.error(f"{func.__name__}: {e}")
                return False, f"{error_message}: {str(e)}", {}
            except Exception as e:
                logging.exception(f"Unexpected error in {func.__name__}")
                return False, f"{error_message}: Unexpected error", {}
        return wrapper
    return decorator

# Usage
@handle_errors("Document processing failed")
def process_documents(files, session_id):
    # Processing logic that might raise exceptions
    pass
```

### 8.3 Testing Framework

#### 8.3.1 Test Structure
```python
# tests/test_web_ui.py
import pytest
import tempfile
import json
from pathlib import Path
from unittest.mock import Mock, patch

from src.web.ui import DocuChatWebInterface
from src.web.session import SessionManager
from src.core.rag_pipeline import RAGPipeline

class TestDocuChatWebInterface:
    """Test suite for web interface functionality"""
    
    @pytest.fixture
    def web_ui(self):
        """Create web UI instance for testing"""
        return DocuChatWebInterface()
    
    @pytest.fixture
    def sample_files(self):
        """Create sample files for upload testing"""
        files = []
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write("Sample document content for testing")
            files.append(Mock(name=Path(f.name).name, read=lambda: b"Sample content"))
        return files
    
    def test_session_creation(self, web_ui):
        """Test session creation functionality"""
        session_id = web_ui.create_session()
        assert isinstance(session_id, str)
        assert len(session_id) > 0
    
    def test_document_upload(self, web_ui, sample_files):
        """Test document upload and processing"""
        session_id = web_ui.create_session()
        
        with patch('src.web.rag_service.WebRAGPipeline') as mock_pipeline:
            mock_pipeline.return_value.process_documents_async.return_value = {
                'success': True, 'message': 'Processing started'
            }
            
            result_message, status = web_ui.upload_and_process(
                sample_files, session_id
            )
            
            assert status == "processing"
            assert "Started processing" in result_message
    
    @pytest.mark.integration
    def test_full_rag_pipeline(self, web_ui, sample_files):
        """Integration test for complete RAG pipeline"""
        # This test requires actual Ollama service running
        session_id = web_ui.create_session()
        
        # Upload and process documents
        web_ui.upload_and_process(sample_files, session_id)
        
        # Wait for processing to complete (in real test, use proper async handling)
        import time
        time.sleep(5)
        
        # Test chat functionality
        response = web_ui.chat_with_documents(
            "What is this document about?", [], session_id
        )
        
        assert response[1]  # Check that history was updated
        assert len(response[1][-1]) == 2  # Question and answer pair
```

#### 8.3.2 Performance Testing
```python
# tests/test_performance.py
import pytest
import time
import concurrent.futures
from src.web.ui import DocuChatWebInterface

class TestPerformance:
    """Performance and load testing"""
    
    def test_concurrent_sessions(self):
        """Test handling of multiple concurrent sessions"""
        web_ui = DocuChatWebInterface()
        
        def create_and_test_session():
            session_id = web_ui.create_session()
            # Simulate some processing
            time.sleep(0.1)
            return session_id
        
        # Test 10 concurrent sessions
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(create_and_test_session) for _ in range(10)]
            session_ids = [f.result() for f in concurrent.futures.as_completed(futures)]
        
        assert len(set(session_ids)) == 10  # All sessions should be unique
    
    @pytest.mark.benchmark
    def test_document_processing_time(self, benchmark):
        """Benchmark document processing time"""
        web_ui = DocuChatWebInterface()
        sample_content = "Sample document content " * 1000  # ~25KB
        
        def process_document():
            # Simulate document processing
            return web_ui._process_single_document(sample_content)
        
        result = benchmark(process_document)
        assert result is not None
```

---

## 9. Testing Strategy

### 9.1 Test Categories

#### 9.1.1 Unit Tests
- **Component Testing**: Individual RAG components (embeddings, vector DB, LLM client)
- **Web Interface Testing**: Gradio component interactions and state management
- **Session Management**: User session lifecycle and data isolation
- **Security Testing**: Input validation, sanitization, and access control

#### 9.1.2 Integration Tests
- **End-to-End Pipeline**: Complete document upload through chat response
- **Database Integration**: ChromaDB operations with session isolation
- **Ollama Integration**: LLM service connectivity and error handling
- **File System Integration**: Document storage and retrieval

#### 9.1.3 Performance Tests
- **Load Testing**: Multiple concurrent users and sessions
- **Stress Testing**: Large document processing and memory usage
- **Response Time Testing**: Query processing speed benchmarks

### 9.2 Test Data Management

#### 9.2.1 Test Document Sets
```python
# tests/test_data.py
import os
from pathlib import Path

class TestDataManager:
    """Manage test documents and data"""
    
    def __init__(self):
        self.test_data_path = Path(__file__).parent / "data"
        self.test_data_path.mkdir(exist_ok=True)
    
    @property
    def sample_documents(self) -> Dict[str, Path]:
        """Get paths to sample test documents"""
        return {
            'small_pdf': self.test_data_path / "sample_small.pdf",
            'large_pdf': self.test_data_path / "sample_large.pdf", 
            'docx': self.test_data_path / "sample.docx",
            'txt': self.test_data_path / "sample.txt",
            'markdown': self.test_data_path / "sample.md"
        }
    
    def create_test_documents(self):
        """Create sample documents for testing"""
        # Create sample text document
        with open(self.sample_documents['txt'], 'w') as f:
            f.write("""
            This is a sample document for testing the DocuChat system.
            It contains multiple paragraphs to test chunking functionality.
            
            The document discusses various topics including:
            - Machine learning and artificial intelligence
            - Natural language processing techniques
            - Document retrieval systems
            - Question answering systems
            
            This content will be used to test the RAG pipeline end-to-end.
            """)
        
        # Create markdown document
        with open(self.sample_documents['markdown'], 'w') as f:
            f.write("""
            # Sample Markdown Document
            
            ## Introduction
            This document tests markdown processing capabilities.
            
            ## Features
            - **Bold text** processing
            - *Italic text* processing
            - `Code snippets`
            
            ## Conclusion
            Markdown documents should be processed correctly.
            """)

test_data = TestDataManager()
```

### 9.3 Automated Testing Pipeline

#### 9.3.1 GitHub Actions Configuration
```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10"]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Set up Ollama (for integration tests)
      run: |
        curl -fsSL https://ollama.ai/install.sh | sh
        ollama serve &
        sleep 10
        ollama pull gemma3:270m
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --timeout=300
    
    - name: Run security tests
      run: |
        bandit -r src/
        safety check
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  performance:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: "3.9"
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest-benchmark
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ --benchmark-only --benchmark-json=benchmark.json
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
```

---

## 10. Performance Considerations

### 10.1 Scalability Design

#### 10.1.1 Session-Based Resource Management
```python
# Resource allocation per session
class SessionResourceManager:
    """Manage computational resources per user session"""
    
    def __init__(self, max_concurrent_sessions: int = 10):
        self.max_concurrent_sessions = max_concurrent_sessions
        self.active_sessions = {}
        self.resource_locks = {}
    
    def allocate_session_resources(self, session_id: str) -> Dict[str, Any]:
        """Allocate computational resources for a session"""
        if len(self.active_sessions) >= self.max_concurrent_sessions:
            # Implement queuing or resource recycling
            self._cleanup_inactive_sessions()
        
        self.active_sessions[session_id] = {
            'allocated_memory': 512 * 1024 * 1024,  # 512MB per session
            'max_processing_time': 300,  # 5 minutes max
            'max_documents': 50,
            'vector_db_instance': None
        }
        
        return self.active_sessions[session_id]
    
    def _cleanup_inactive_sessions(self):
        """Clean up resources from inactive sessions"""
        cutoff_time = time.time() - 3600  # 1 hour timeout
        
        expired_sessions = [
            sid for sid, info in self.active_sessions.items()
            if info.get('last_activity', 0) < cutoff_time
        ]
        
        for session_id in expired_sessions:
            self.deallocate_session_resources(session_id)
```

#### 10.1.2 Caching Strategy
```python
# Multi-level caching implementation
import redis
from functools import lru_cache
import pickle

class CacheManager:
    """Multi-level caching for performance optimization"""
    
    def __init__(self, redis_url: Optional[str] = None):
        # L1 Cache: In-memory LRU cache
        self.memory_cache = {}
        self.max_memory_items = 1000
        
        # L2 Cache: Redis (if available)
        self.redis_client = None
        if redis_url:
            try:
                self.redis_client = redis.from_url(redis_url)
                self.redis_client.ping()
            except Exception:
                self.redis_client = None
    
    def get_embedding_cache_key(self, text: str, model_name: str) -> str:
        """Generate cache key for embeddings"""
        import hashlib
        content_hash = hashlib.md5(text.encode()).hexdigest()
        return f"embedding:{model_name}:{content_hash}"
    
    def get_cached_embedding(self, text: str, model_name: str) -> Optional[np.ndarray]:
        """Retrieve cached embedding"""
        cache_key = self.get_embedding_cache_key(text, model_name)
        
        # Check L1 cache first
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # Check L2 cache (Redis)
        if self.redis_client:
            try:
                cached_data = self.redis_client.get(cache_key)
                if cached_data:
                    embedding = pickle.loads(cached_data)
                    # Promote to L1 cache
                    self._store_in_memory_cache(cache_key, embedding)
                    return embedding
            except Exception:
                pass
        
        return None
    
    def store_cached_embedding(self, text: str, model_name: str, embedding: np.ndarray):
        """Store embedding in cache"""
        cache_key = self.get_embedding_cache_key(text, model_name)
        
        # Store in L1 cache
        self._store_in_memory_cache(cache_key, embedding)
        
        # Store in L2 cache (Redis) with expiration
        if self.redis_client:
            try:
                self.redis_client.setex(
                    cache_key, 
                    86400,  # 24 hours
                    pickle.dumps(embedding)
                )
            except Exception:
                pass
    
    def _store_in_memory_cache(self, key: str, value: Any):
        """Store item in memory cache with LRU eviction"""
        if len(self.memory_cache) >= self.max_memory_items:
            # Remove oldest item (simple FIFO for now)
            oldest_key = next(iter(self.memory_cache))
            del self.memory_cache[oldest_key]
        
        self.memory_cache[key] = value
```

### 10.2 Optimization Strategies

#### 10.2.1 Batch Processing
```python
# Efficient batch processing for embeddings and chunking
class BatchProcessor:
    """Optimize processing through intelligent batching"""
    
    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
    
    async def process_documents_in_batches(self, documents: List[Dict]) -> List[Dict]:
        """Process documents in optimized batches"""
        results = []
        
        # Group documents by type for optimal processing
        documents_by_type = self._group_by_type(documents)
        
        for doc_type, docs in documents_by_type.items():
            # Process each type in batches
            for i in range(0, len(docs), self.batch_size):
                batch = docs[i:i + self.batch_size]
                
                # Process batch with type-specific optimization
                batch_results = await self._process_batch(doc_type, batch)
                results.extend(batch_results)
        
        return results
    
    def _group_by_type(self, documents: List[Dict]) -> Dict[str, List[Dict]]:
        """Group documents by file type for batch processing"""
        groups = {}
        for doc in documents:
            file_type = Path(doc['filename']).suffix.lower()
            if file_type not in groups:
                groups[file_type] = []
            groups[file_type].append(doc)
        return groups
    
    async def _process_batch(self, doc_type: str, batch: List[Dict]) -> List[Dict]:
        """Process a batch of documents of the same type"""
        if doc_type == '.pdf':
            return await self._process_pdf_batch(batch)
        elif doc_type in ['.docx', '.doc']:
            return await self._process_docx_batch(batch)
        else:
            return await self._process_text_batch(batch)
```

#### 10.2.2 Async Processing Implementation
```python
# Asynchronous processing for better responsiveness
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncDocumentProcessor:
    """Asynchronous document processing pipeline"""
    
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.embedding_semaphore = asyncio.Semaphore(2)  # Limit concurrent embeddings
    
    async def process_document_async(self, file_path: str, session_id: str) -> Dict[str, Any]:
        """Process single document asynchronously"""
        try:
            # Step 1: Load document (I/O bound - run in thread)
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                self.executor, self._load_document_sync, file_path
            )
            
            if not content:
                return {'success': False, 'error': 'Failed to load document'}
            
            # Step 2: Chunk document (CPU bound but fast - run in thread)
            chunks = await loop.run_in_executor(
                self.executor, self._chunk_document_sync, content, file_path
            )
            
            # Step 3: Generate embeddings (CPU bound - semaphore limited)
            async with self.embedding_semaphore:
                embeddings = await loop.run_in_executor(
                    self.executor, self._generate_embeddings_sync, chunks
                )
            
            # Step 4: Store in vector database (I/O bound)
            success = await loop.run_in_executor(
                self.executor, self._store_embeddings_sync, embeddings, session_id
            )
            
            return {
                'success': success,
                'chunks_processed': len(chunks),
                'embeddings_generated': len(embeddings)
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _load_document_sync(self, file_path: str) -> Optional[str]:
        """Synchronous document loading (runs in thread)"""
        from document_loader import DocumentLoader
        loader = DocumentLoader(verbose=False)
        result = loader.load_document(file_path)
        return result[1] if result else None
    
    def _chunk_document_sync(self, content: str, source_file: str) -> List[Dict]:
        """Synchronous document chunking (runs in thread)"""
        from chunker import DocumentChunker
        chunker = DocumentChunker(chunk_size=2000, chunk_overlap=200)
        return list(chunker.chunk_document(content, source_file))
    
    def _generate_embeddings_sync(self, chunks: List[Dict]) -> List[Dict]:
        """Synchronous embedding generation (runs in thread)"""
        from embeddings import EmbeddingGenerator
        generator = EmbeddingGenerator(model_name="all-MiniLM-L6-v2")
        return generator.embed_chunks(chunks)
```

### 10.3 Memory Management

#### 10.3.1 Resource Monitoring
```python
# Real-time resource monitoring and management
import psutil
import gc
from typing import Dict, Any

class ResourceMonitor:
    """Monitor and manage system resources"""
    
    def __init__(self, memory_limit_gb: float = 4.0):
        self.memory_limit = memory_limit_gb * 1024 * 1024 * 1024  # Convert to bytes
        self.process = psutil.Process()
    
    def check_memory_usage(self) -> Dict[str, Any]:
        """Check current memory usage"""
        memory_info = self.process.memory_info()
        memory_percent = self.process.memory_percent()
        
        return {
            'memory_used_bytes': memory_info.rss,
            'memory_used_mb': memory_info.rss / 1024 / 1024,
            'memory_percent': memory_percent,
            'memory_available': self.memory_limit - memory_info.rss,
            'is_over_limit': memory_info.rss > self.memory_limit
        }
    
    def cleanup_if_needed(self) -> bool:
        """Perform cleanup if memory usage is high"""
        memory_status = self.check_memory_usage()
        
        if memory_status['memory_percent'] > 80:
            # Force garbage collection
            gc.collect()
            
            # Clear caches
            self._clear_caches()
            
            return True
        
        return False
    
    def _clear_caches(self):
        """Clear various application caches"""
        # Clear embedding cache
        if hasattr(self, 'embedding_cache'):
            self.embedding_cache.clear()
        
        # Clear session caches
        for session in self.active_sessions.values():
            if hasattr(session, 'cache'):
                session.cache.clear()
        
        # Force garbage collection again
        gc.collect()
```

---

## Conclusion

This comprehensive documentation provides a complete roadmap for transforming the DocuChat CLI-based RAG system into a production-ready web application using Gradio. The phased implementation approach ensures systematic development while maintaining the robustness and functionality of the existing system.

### Key Success Factors

1. **Incremental Development**: Three-phase approach minimizes risk and allows for iterative improvement
2. **Component Reuse**: Maximum utilization of existing, tested RAG components
3. **Security First**: Comprehensive security measures from the ground up
4. **Performance Focus**: Optimized architecture for scalability and responsiveness
5. **Production Ready**: Complete deployment, monitoring, and maintenance strategies

### Next Steps

1. **Phase 1 Implementation**: Begin with core web interface and document upload functionality
2. **Testing and Validation**: Comprehensive testing at each phase
3. **Security Review**: Thorough security assessment before production deployment
4. **Performance Optimization**: Continuous monitoring and optimization
5. **Documentation Updates**: Keep documentation current with implementation changes

The resulting web application will provide users with an intuitive, powerful interface for document-based question answering while maintaining the robustness and accuracy of the underlying RAG system.

---

*This documentation serves as both an implementation guide and future reference for the DocuChat Web UI project. All code examples and architectural decisions are based on analysis of the existing DocuChat codebase and industry best practices for production web applications.*